{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37264bitenvvenvbfb87bad6a704ed3bff22befd15a1249",
   "display_name": "Python 3.7.2 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of longest common subsequence (LCS) algorithms\n",
    "\n",
    "This notebooks compares longest common subsequence (LCS) algorithms:\n",
    "\n",
    "- Brute-force: generate combinations of subseqences and check if they are common subsequences.\n",
    "- Dynamic programming: take advantage of common subproblems to not evaluate the same subsequence more than once.\n",
    "- Hirschbger's linear space: a dynamic programming approach that uses significantly less memory.\n",
    "\n",
    "The comparison measures:\n",
    "\n",
    "- Runtime efficiency: how long does it take to find the LCS.\n",
    "- Memory efficiency: how much memory is used to find the LCS."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    ">> Add here the problem description and references."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook structure\n",
    "\n",
    ">>> Describe here the structure of the notebook"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check and initialization\n",
    "\n",
    "Check that the algorithms work by testing them against controlled input.\n",
    "\n",
    "There are three part to the tests:\n",
    "\n",
    "1. Automated tests that check against well-defined inputs. They are meant to be easy to debug, in case an algorithm fails.\n",
    "1. Tests with longer inputs that simular DNA strands. They test more realistic scenarios, but still short enough to run fast.\n",
    "1. A visual check, by printing the aligned subsequence. They guard against the test code itself having a failure that generates false positives."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lcs_test\n",
    "\n",
    "lcs_test.test(visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a seed to make pseudo-random generator generate the same sequence across runs. This makes it easier to compare different runs of the algorithms."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "To illustrate a real-life scenario, the code checks if a DNA strand is part of\n",
    "a larger DNA sequence (see [this for an illustration](https://en.wikipedia.org/wiki/Subsequence#Applications))."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime tests and analysis"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "\n",
    "Get raw test data. This includes all repetitions.Get raw test data. This includes all repetitions."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_results_raw = metrics.runtime(verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change into a Pandas dataframe to facilitate analysis.\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rt_results = pd.DataFrame(rt_results_raw)\n",
    "rt_results.columns = ['Algorithm', 'DNA size', 'Strand size', 'Test number', 'Runtime (s)']\n",
    "display(rt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_results_summary = rt_results.groupby(['Algorithm','DNA size', 'Strand size']).mean()\n",
    "rt_results_summary.drop(['Test number'], axis='columns', inplace=True)\n",
    "rt_results_summary.reset_index(inplace=True)\n",
    "display(rt_results_summary.sort_values(by=['DNA size', 'Runtime (s)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.barplot(data=rt_results_summary, y='DNA size', x='Runtime (s)', hue='Algorithm',\n",
    "    orient='h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage and analysis\n",
    "\n",
    "Get raw test data. This includes all repetitions."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_results_raw = metrics.memory(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_results = pd.DataFrame(mem_results_raw)\n",
    "mem_results.columns = ['Algorithm', 'DNA size', 'Strand size', 'Test number', 'Memory used (KiB)', 'Runtime (s)']\n",
    "display(mem_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "| <!-- -->    | <!-- -->    |\n",
    "|-------------|-------------|\n",
    "| 1  | CLRS  |"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}